{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBLi69V0yECT",
    "outputId": "8b6a228c-03bc-41ee-e5e5-126635e6d112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "0% [Working]\r\n",
      "            \r\n",
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "\r\n",
      "0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 12.7 kB/129 kB 10%] [Connected t\r\n",
      "                                                                                                    \r\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "\r\n",
      "0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 69.2 kB/129 kB 54%] [2 InRelease\r\n",
      "0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 95.3 kB/129 kB 74%] [Waiting for\r\n",
      "0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcon\r\n",
      "                                                                                                    \r\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "\r\n",
      "0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.\r\n",
      "                                                                                                    \r\n",
      "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,224 kB]\n",
      "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,506 kB]\n",
      "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,619 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,513 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,736 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
      "Fetched 19.5 MB in 2s (8,300 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "#pyspark setup\n",
    "\n",
    "!apt-get update\n",
    "# Install Java 8 (required by Spark)\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Install Spark\n",
    "!pip install pyspark\n",
    "\n",
    "# setup environment variables\n",
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.10/dist-packages/pyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOvkex3d141Q",
    "outputId": "85ca6a1d-798c-41bb-e75d-fda76e5108af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Retail Sales Analytics\").getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "sales_df = spark.read.csv(\"/Online Retail .csv\", header=True, inferSchema= True) # Corrected the filename\n",
    "reviews_df = spark.read.csv(\"/reviews.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Explore data\n",
    "sales_df.show(10)\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Check schema\n",
    "sales_df.printSchema()\n",
    "reviews_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttJU5EDGE9Jd",
    "outputId": "bb14d039-7412-4fb6-8626-a86540185825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Sales Data:\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "|    false|    false|      false|   false|      false|    false|     false|  false|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Missing Values in Reviews Data:\n",
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "| false|    false|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"Missing Values in Sales Data:\")\n",
    "sales_df.select([col(column).isNull().alias(column) for column in sales_df.columns]).show()\n",
    "\n",
    "print(\"Missing Values in Reviews Data:\")\n",
    "reviews_df.select([col(column).isNull().alias(column) for column in reviews_df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aj9-PeLtdz-Y",
    "outputId": "129b864b-e503-40e7-a732-3a7c6953d10c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------------------+--------+-----------+---------+-------------------+-------+\n",
      "|summary|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|         CustomerID|Country|\n",
      "+-------+---------+---------+--------------------+--------+-----------+---------+-------------------+-------+\n",
      "|  count|   541909|   541909|              541909|  541909|     541909|   541909|             541909| 541909|\n",
      "|   mean|      0.0|      0.0|0.002683107311375157|     0.0|        0.0|      0.0|  0.249266943342886|    0.0|\n",
      "| stddev|      0.0|      0.0| 0.05172922949811863|     0.0|        0.0|      0.0|0.43258904241984053|    0.0|\n",
      "|    min|        0|        0|                   0|       0|          0|        0|                  0|      0|\n",
      "|    25%|        0|        0|                   0|       0|          0|        0|                  0|      0|\n",
      "|    50%|        0|        0|                   0|       0|          0|        0|                  0|      0|\n",
      "|    75%|        0|        0|                   0|       0|          0|        0|                  0|      0|\n",
      "|    max|        0|        0|                   1|       0|          0|        0|                  1|      0|\n",
      "+-------+---------+---------+--------------------+--------+-----------+---------+-------------------+-------+\n",
      "\n",
      "+-------+------+---------+\n",
      "|summary|Review|Sentiment|\n",
      "+-------+------+---------+\n",
      "|  count|   386|      386|\n",
      "|   mean|   0.0|      0.0|\n",
      "| stddev|   0.0|      0.0|\n",
      "|    min|     0|        0|\n",
      "|    25%|     0|        0|\n",
      "|    50%|     0|        0|\n",
      "|    75%|     0|        0|\n",
      "|    max|     0|        0|\n",
      "+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select([(col(c).isNull().cast(\"int\")).alias(c) for c in sales_df.columns]).summary().show()\n",
    "reviews_df.select([(col(c).isNull().cast(\"int\")).alias(c) for c in reviews_df.columns]).summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaeCVOXsg7N2",
    "outputId": "8cf0fdf9-7e61-4f13-f27e-b3928d9d4f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset:\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n",
      "Handling missing values...\n",
      "Dataset after dropping missing values:\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Filtering non-meaningful text...\n",
      "Dataset after filtering non-meaningful text:\n",
      "+--------------------+---------+\n",
      "|              Review|Sentiment|\n",
      "+--------------------+---------+\n",
      "|This product exce...| Positive|\n",
      "|The product was d...|  Neutral|\n",
      "|I had a terrible ...| Negative|\n",
      "|It's an okay prod...|  Neutral|\n",
      "|Disappointed with...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|I had a terrible ...| Negative|\n",
      "|Avoid this compan...| Negative|\n",
      "|This product exce...| Positive|\n",
      "|This product is o...| Positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Validating sentiment values...\n",
      "Dataset after validating sentiment:\n",
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "+------+---------+\n",
      "\n",
      "Dropping duplicates...\n",
      "Final cleaned dataset:\n",
      "+------+---------+\n",
      "|Review|Sentiment|\n",
      "+------+---------+\n",
      "+------+---------+\n",
      "\n",
      "Cleaned dataset saved to cleaned_reviews_data.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Data Cleansing - Simplified Reviews\").getOrCreate()\n",
    "\n",
    "# Load the review dataset\n",
    "reviews_data_path = \"/reviews.csv\"  # Replace with the actual file path\n",
    "reviews_df = spark.read.csv(reviews_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect data\n",
    "print(\"Initial dataset:\")\n",
    "reviews_df.show(10)\n",
    "reviews_df.printSchema()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "reviews_df = reviews_df.dropna(subset=[\"Review\", \"Sentiment\"])\n",
    "\n",
    "# Debugging step: Check dataset after dropping missing values\n",
    "print(\"Dataset after dropping missing values:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Remove rows with empty or whitespace-only reviewText\n",
    "print(\"Filtering non-meaningful text...\")\n",
    "reviews_df = reviews_df.filter(col(\"Review\").rlike(r\"\\S+\"))\n",
    "\n",
    "# Debugging step: Check dataset after filtering non-meaningful text\n",
    "print(\"Dataset after filtering non-meaningful text:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Validate sentiment values (should be \"positive\" or \"negative\")\n",
    "print(\"Validating sentiment values...\")\n",
    "valid_sentiments = [\"positive\", \"negative\"]\n",
    "reviews_df = reviews_df.filter(col(\"Sentiment\").isin(valid_sentiments))\n",
    "\n",
    "# Debugging step: Check dataset after validating sentiment\n",
    "print(\"Dataset after validating sentiment:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Drop duplicates\n",
    "print(\"Dropping duplicates...\")\n",
    "reviews_df = reviews_df.dropDuplicates()\n",
    "\n",
    "# Final dataset preview\n",
    "print(\"Final cleaned dataset:\")\n",
    "reviews_df.show(10)\n",
    "\n",
    "# Save cleaned data for further use\n",
    "output_path = \"cleaned_reviews_data.csv\"  # Replace with your desired output path\n",
    "reviews_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "print(f\"Cleaned dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT3sqj7Lminf",
    "outputId": "d0a33d02-ec54-4330-fb00-cd8dc4c1c960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536375|   84406B|CREAM CUPID HEART...|       8| 2010-01-12|     2.75|     17850|United Kingdom|\n",
      "|   536389|   85014A|BLACK/BLUE POLKAD...|       3| 2010-01-12|     5.95|     12431|     Australia|\n",
      "|   536408|    22492|MINI PAINT SET VI...|      36| 2010-01-12|     0.65|     14307|United Kingdom|\n",
      "|   536425|    22829|SWEETHEART WIRE W...|       2| 2010-01-12|     9.95|     13758|United Kingdom|\n",
      "|   536437|    22189|CREAM HEART CARD ...|      72| 2010-01-12|     3.39|     13694|United Kingdom|\n",
      "|   536528|   85114C|RED ENCHANTED FOR...|       1| 2010-01-12|     1.65|     15525|United Kingdom|\n",
      "|   536532|    22555|PLASTERS IN TIN S...|      36| 2010-01-12|     1.65|     12433|        Norway|\n",
      "|   536532|    21786|  POLKADOT RAIN HAT |      24| 2010-01-12|     0.42|     12433|        Norway|\n",
      "|   536562|   79302M|ART LIGHTS,FUNK M...|       6| 2010-01-12|     2.95|     13468|United Kingdom|\n",
      "|   536569|    22825|DECORATIVE PLANT ...|       1| 2010-01-12|     7.95|     16274|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Retail Sales Analytics\").getOrCreate()\n",
    "\n",
    "# Set legacy time parser policy to handle the problematic date format\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Load datasets\n",
    "sales_df = spark.read.csv(\"/Online Retail .csv\", header=True, inferSchema= True) # Corrected the filename\n",
    "\n",
    "\n",
    "# Inspect data\n",
    "sales_df.show(10)\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Handle missing values\n",
    "# Drop rows with null CustomerID or Description, as they are critical fields\n",
    "sales_df = sales_df.dropna(subset=[\"StockCode\", \"Description\"])\n",
    "\n",
    "# Drop duplicates\n",
    "sales_df = sales_df.dropDuplicates()\n",
    "\n",
    "# Parse InvoiceDate to a proper date format\n",
    "# Use the correct format based on your data (e.g., \"dd/MM/yy HH:mm\")\n",
    "sales_df = sales_df.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"dd/MM/yy HH:mm\"))\n",
    "\n",
    "# Filter out invalid data (e.g., Quantity or UnitPrice less than or equal to 0)\n",
    "sales_df = sales_df.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
    "\n",
    "# Final schema and data preview\n",
    "sales_df.printSchema()\n",
    "sales_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGgSRfRJrcqd"
   },
   "outputs": [],
   "source": [
    "# Save cleaned datasets for downstream tasks\n",
    "sales_df.write.csv(\"cleaned_sales_data.csv\", header=True, mode=\"overwrite\")\n",
    "reviews_df.write.csv(\"cleaned_reviews_data.csv\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8y5X8p0DvVLY",
    "outputId": "3bb1d8d1-d268-46c1-f7c1-1b0bf9893752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset:\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity| InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/10 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/10 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/10 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/10 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/10 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/10 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/10 8:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "Calculating total sales per product per month...\n",
      "+----+-----+---------+--------------------+-----------------+---------------------+\n",
      "|Year|Month|StockCode|         Description|TotalQuantitySold|TotalRevenueGenerated|\n",
      "+----+-----+---------+--------------------+-----------------+---------------------+\n",
      "|NULL| NULL|    10002|INFLATABLE POLITI...|              860|    759.8900000000002|\n",
      "|NULL| NULL|    10002|                NULL|              177|                  0.0|\n",
      "|NULL| NULL|    10080|GROOVY CACTUS INF...|              303|   119.08999999999999|\n",
      "|NULL| NULL|    10080|                NULL|              170|                  0.0|\n",
      "|NULL| NULL|    10080|               check|               22|                  0.0|\n",
      "|NULL| NULL|    10120|        DOGGY RUBBER|              193|   40.530000000000015|\n",
      "|NULL| NULL|   10123C|                NULL|              -18|                  0.0|\n",
      "|NULL| NULL|   10123C|HEARTS WRAPPING T...|                5|                 3.25|\n",
      "|NULL| NULL|   10123G|                NULL|              -38|                  0.0|\n",
      "|NULL| NULL|   10124A|SPOTS ON RED BOOK...|               16|                 6.72|\n",
      "+----+-----+---------+--------------------+-----------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Calculating average revenue per customer segment...\n",
      "+------------------+--------------------+\n",
      "|           Country|AvgRevenuePerCountry|\n",
      "+------------------+--------------------+\n",
      "|            Sweden|   79.21192640692637|\n",
      "|         Singapore|    39.8270305676856|\n",
      "|           Germany|   23.34894260136918|\n",
      "|            France|  23.069288301975035|\n",
      "|            Greece|   32.26383561643835|\n",
      "|European Community|   21.17622950819672|\n",
      "|           Belgium|  19.773301111648145|\n",
      "|           Finland|  32.124805755395684|\n",
      "|             Malta|   19.72811023622048|\n",
      "|       Unspecified|  10.649753363228692|\n",
      "|             Italy|  21.034259028642584|\n",
      "|              EIRE|   32.12259882869682|\n",
      "|         Lithuania|   47.45885714285714|\n",
      "|            Norway|   32.37887661141805|\n",
      "|             Spain|  21.624390051322607|\n",
      "|           Denmark|  48.247146529562954|\n",
      "|         Hong Kong|    35.1286111111111|\n",
      "|           Iceland|  23.681318681318682|\n",
      "|            Israel|  26.625656565656566|\n",
      "|   Channel Islands|  26.499063324538245|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Analyzing seasonal patterns for top-selling products...\n",
      "+---------+--------------------+----------+\n",
      "|StockCode|         Description|TotalSales|\n",
      "+---------+--------------------+----------+\n",
      "|    84077|WORLD WAR 2 GLIDE...|     53847|\n",
      "|   85099B|JUMBO BAG RED RET...|     47363|\n",
      "|    84879|ASSORTED COLOUR B...|     36381|\n",
      "|    22197|      POPCORN HOLDER|     36334|\n",
      "|    21212|PACK OF 72 RETROS...|     36039|\n",
      "+---------+--------------------+----------+\n",
      "\n",
      "+---------+--------------------+-----+------------+\n",
      "|StockCode|         Description|Month|MonthlySales|\n",
      "+---------+--------------------+-----+------------+\n",
      "|    21212|PACK OF 72 RETROS...| NULL|       36039|\n",
      "|    22197|      POPCORN HOLDER| NULL|       36334|\n",
      "|    84077|WORLD WAR 2 GLIDE...| NULL|       53847|\n",
      "|    84879|ASSORTED COLOUR B...| NULL|       36381|\n",
      "|   85099B|JUMBO BAG RED RET...| NULL|       47363|\n",
      "+---------+--------------------+-----+------------+\n",
      "\n",
      "Feature engineering...\n",
      "+----------+---------------------+\n",
      "|CustomerID|CustomerLifetimeValue|\n",
      "+----------+---------------------+\n",
      "|     17420|    598.8300000000002|\n",
      "|     16861|               151.65|\n",
      "|     16503|   1421.4300000000003|\n",
      "|     15727|    5178.959999999999|\n",
      "|     17389|             31300.08|\n",
      "|     15447|               155.17|\n",
      "|     14450|               483.25|\n",
      "|     13623|               672.44|\n",
      "|     13285|   2709.1199999999994|\n",
      "|     16339|   109.95000000000002|\n",
      "+----------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+--------------------+--------------+------------------+----------------------+\n",
      "|StockCode|         Description|TotalUnitsSold|      TotalRevenue|ProductPopularityScore|\n",
      "+---------+--------------------+--------------+------------------+----------------------+\n",
      "|   84279P|CHERRY BLOSSOM  D...|           364|1516.9599999999998|             552173.44|\n",
      "|    85015|SET OF 12  VINTAG...|          2182|1719.8400000000006|    3752690.8800000013|\n",
      "|    21249|WOODLAND  HEIGHT ...|           724|2160.0000000000005|    1563840.0000000002|\n",
      "|    21002|ROSE DU SUD DRAWS...|           125| 428.6600000000002|     53582.50000000002|\n",
      "|    84987|SET OF 36 TEATIME...|          3038| 4452.320000000005|  1.3526148160000015E7|\n",
      "|    20671|BLUE TEATIME PRIN...|            10|              12.5|                 125.0|\n",
      "|    22690|DOORMAT HOME SWEE...|          1617| 9911.299999999994|   1.602657209999999E7|\n",
      "|    22708|     WRAP DOLLY GIRL|          2775|            1149.5|             3189862.5|\n",
      "|   90184A|AMBER CHUNKY BEAD...|            13|117.73000000000003|    1530.4900000000005|\n",
      "|    21285|RETROSPOT CANDLE ...|           341|398.28000000000003|             135813.48|\n",
      "+---------+--------------------+--------------+------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+-----------------+\n",
      "|Month|TotalMonthlySales|\n",
      "+-----+-----------------+\n",
      "| NULL|          5176450|\n",
      "+-----+-----------------+\n",
      "\n",
      "Results saved to output_sales_aggregation\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, month, year, count, expr\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Sales Aggregation and Feature Engineering\").getOrCreate()\n",
    "\n",
    "# Load the sales dataset\n",
    "sales_df = spark.read.csv(\"/content/Online Retail.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Inspect dataset\n",
    "print(\"Initial dataset:\")\n",
    "sales_df.show(10)\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Ensure `InvoiceDate` is in proper datetime format\n",
    "sales_df = sales_df.withColumn(\"InvoiceDate\", col(\"InvoiceDate\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate revenue for each transaction\n",
    "sales_df = sales_df.withColumn(\"RevenueGenerated\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
    "\n",
    "# Extract month and year for analysis\n",
    "sales_df = sales_df.withColumn(\"Year\", year(col(\"InvoiceDate\"))).withColumn(\"Month\", month(col(\"InvoiceDate\")))\n",
    "\n",
    "# Total sales per product per month\n",
    "print(\"Calculating total sales per product per month...\")\n",
    "total_sales_per_product = sales_df.groupBy(\"Year\", \"Month\", \"StockCode\", \"Description\") \\\n",
    "    .agg(\n",
    "        sum(\"Quantity\").alias(\"TotalQuantitySold\"),\n",
    "        sum(\"RevenueGenerated\").alias(\"TotalRevenueGenerated\")\n",
    "    ) \\\n",
    "    .orderBy(\"Year\", \"Month\", \"StockCode\")\n",
    "\n",
    "total_sales_per_product.show(10)\n",
    "\n",
    "# Average revenue per customer segment (Country used as proxy for customer segments)\n",
    "print(\"Calculating average revenue per customer segment...\")\n",
    "avg_revenue_per_country = sales_df.groupBy(\"Country\") \\\n",
    "    .agg(avg(\"RevenueGenerated\").alias(\"AvgRevenuePerCountry\"))\n",
    "\n",
    "avg_revenue_per_country.show()\n",
    "\n",
    "# Seasonal patterns for top-selling products\n",
    "print(\"Analyzing seasonal patterns for top-selling products...\")\n",
    "# Identify top-selling products by total sales\n",
    "top_products = total_sales_per_product.groupBy(\"StockCode\", \"Description\") \\\n",
    "    .agg(sum(\"TotalQuantitySold\").alias(\"TotalSales\")) \\\n",
    "    .orderBy(col(\"TotalSales\").desc()) \\\n",
    "    .limit(5)  # Top 5 products\n",
    "\n",
    "top_products.show()\n",
    "\n",
    "# Seasonal trends for top products\n",
    "seasonal_trends = total_sales_per_product.join(top_products, [\"StockCode\", \"Description\"]) \\\n",
    "    .groupBy(\"StockCode\", \"Description\", \"Month\") \\\n",
    "    .agg(sum(\"TotalQuantitySold\").alias(\"MonthlySales\")) \\\n",
    "    .orderBy(\"StockCode\", \"Month\")\n",
    "\n",
    "seasonal_trends.show()\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"Feature engineering...\")\n",
    "\n",
    "# Customer Lifetime Value (CLV): Total revenue generated by each customer\n",
    "clv = sales_df.groupBy(\"CustomerID\") \\\n",
    "    .agg(sum(\"RevenueGenerated\").alias(\"CustomerLifetimeValue\"))\n",
    "\n",
    "clv.show(10)\n",
    "\n",
    "# Product Popularity Score: Weighted by total sales and revenue\n",
    "product_popularity = sales_df.groupBy(\"StockCode\", \"Description\") \\\n",
    "    .agg(\n",
    "        sum(\"Quantity\").alias(\"TotalUnitsSold\"),\n",
    "        sum(\"RevenueGenerated\").alias(\"TotalRevenue\")\n",
    "    ) \\\n",
    "    .withColumn(\"ProductPopularityScore\", col(\"TotalUnitsSold\") * col(\"TotalRevenue\"))\n",
    "\n",
    "product_popularity.show(10)\n",
    "\n",
    "# Seasonal Trends: Sales aggregated by month across all products\n",
    "seasonal_trends_all = sales_df.groupBy(\"Month\") \\\n",
    "    .agg(sum(\"Quantity\").alias(\"TotalMonthlySales\")) \\\n",
    "    .orderBy(\"Month\")\n",
    "\n",
    "seasonal_trends_all.show()\n",
    "\n",
    "# Save results for further analysis\n",
    "output_path = \"output_sales_aggregation\"\n",
    "total_sales_per_product.write.csv(f\"{output_path}/total_sales_per_product\", header=True, mode=\"overwrite\")\n",
    "avg_revenue_per_country.write.csv(f\"{output_path}/avg_revenue_per_country\", header=True, mode=\"overwrite\")\n",
    "top_products.write.csv(f\"{output_path}/top_products\", header=True, mode=\"overwrite\")\n",
    "seasonal_trends.write.csv(f\"{output_path}/seasonal_trends\", header=True, mode=\"overwrite\")\n",
    "clv.write.csv(f\"{output_path}/customer_lifetime_value\", header=True, mode=\"overwrite\")\n",
    "product_popularity.write.csv(f\"{output_path}/product_popularity\", header=True, mode=\"overwrite\")\n",
    "seasonal_trends_all.write.csv(f\"{output_path}/seasonal_trends_all\", header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "6Na9JrBRr3BR",
    "outputId": "1e21d7f7-9fcb-4300-a660-c3978a19eaef"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lag() got an unexpected keyword argument 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-72fbb89a309a>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Create 3 lag features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     filtered_trends = filtered_trends.withColumn(f\"lag_{lag}\",\n\u001b[0;32m---> 29\u001b[0;31m                                                  F.lag(\"TotalQuantitySold\", count=lag).over(\n\u001b[0m\u001b[1;32m     30\u001b[0m                                                      Window.partitionBy(\"StockCode\").orderBy(\"Year\", \"Month\")))\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuncT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: lag() got an unexpected keyword argument 'count'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Step 1: Prepare Data\n",
    "# Seasonal trends data from Task 2\n",
    "seasonal_trends = sales_df.groupBy(\"StockCode\", \"Year\", \"Month\") \\\n",
    "    .agg(\n",
    "        F.sum(\"Quantity\").alias(\"TotalQuantitySold\"),\n",
    "        F.sum(\"RevenueGenerated\").alias(\"TotalRevenueGenerated\")\n",
    "    ) \\\n",
    "    .orderBy(\"Year\", \"Month\")\n",
    "\n",
    "# Filter for top products\n",
    "top_products = seasonal_trends.groupBy(\"StockCode\") \\\n",
    "    .agg(F.sum(\"TotalQuantitySold\").alias(\"TotalSales\")) \\\n",
    "    .orderBy(F.col(\"TotalSales\").desc()) \\\n",
    "    .limit(3)\n",
    "\n",
    "# Join to filter only top products\n",
    "filtered_trends = seasonal_trends.join(top_products, on=\"StockCode\", how=\"inner\")\n",
    "\n",
    "# Create Lag Features\n",
    "for lag in range(1, 4):  # Create 3 lag features\n",
    "    filtered_trends = filtered_trends.withColumn(f\"lag_{lag}\",\n",
    "                                                 F.lag(\"TotalQuantitySold\", count=lag).over(\n",
    "                                                     Window.partitionBy(\"StockCode\").orderBy(\"Year\", \"Month\")))\n",
    "\n",
    "# Drop rows with null lag values (start of time series)\n",
    "filtered_trends = filtered_trends.dropna()\n",
    "\n",
    "# Step 2: Assemble Features\n",
    "feature_cols = [f\"lag_{lag}\" for lag in range(1, 4)]  # Lagged features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Step 3: Train/Test Split\n",
    "train_data, test_data = filtered_trends.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 4: Model Pipeline\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=\"TotalQuantitySold\", predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "\n",
    "# Step 5: Hyperparameter Tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"TotalQuantitySold\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "cross_validator = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Train the Model\n",
    "cv_model = cross_validator.fit(train_data)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "# Calculate RMSE and MAE\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Show Predictions\n",
    "predictions.select(\"StockCode\", \"Year\", \"Month\", \"TotalQuantitySold\", \"prediction\").show()\n",
    "\n",
    "# Save the Model\n",
    "cv_model.bestModel.write().overwrite().save(\"output_sales_forecasting_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwayHXYGASTT"
   },
   "source": [
    "TASK 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "-Ht0bNE0AVDL",
    "outputId": "36dda0d7-3270-4197-ae08-26d5aa390352"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'filtered_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e7d7adacbd0a>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Optional: Generate n-grams (bigram as an example)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array<string>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mreviews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ngrams\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Count Vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3127\u001b[0m         \"\"\"\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3130\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'filtered_tokens'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load Review Dataset\n",
    "# Assuming `reviews_df` contains columns \"text\" (review text) and \"sentiment\" (label: 0 for negative, 1 for positive)\n",
    "reviews_df = spark.read.csv(\"/reviews.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 1: Text Preprocessing\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "\n",
    "# Remove stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "\n",
    "# Create n-grams\n",
    "# Optional: Generate n-grams (bigram as an example)\n",
    "ngram = F.udf(lambda tokens: [\" \".join(tokens[i:i+2]) for i in range(len(tokens)-1)], \"array<string>\")\n",
    "reviews_df = reviews_df.withColumn(\"ngrams\", ngram(reviews_df.filtered_tokens))\n",
    "\n",
    "# Count Vectorizer\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"raw_features\")\n",
    "\n",
    "# TF-IDF Transformation\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Step 2: Sentiment Model Training\n",
    "# Logistic Regression Classifier\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=10)\n",
    "\n",
    "# Pipeline for Preprocessing and Model\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, lr])\n",
    "\n",
    "# Train-Test Split\n",
    "train_data, test_data = reviews_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit the Pipeline\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Predictions on Test Data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate Model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Step 3: Sentiment Scoring for Each Product\n",
    "# Assuming `reviews_df` has an additional `product_id` column\n",
    "sentiment_scores = predictions.groupBy(\"product_id\") \\\n",
    "    .agg(F.mean(F.col(\"prediction\")).alias(\"average_sentiment_score\"))\n",
    "\n",
    "# Save Sentiment Scores\n",
    "sentiment_scores.write.csv(\"output_sentiment_scores\", header=True)\n",
    "\n",
    "# Show Sentiment Scores\n",
    "sentiment_scores.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/02 11:17:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/02 11:17:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/02 11:17:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/12/02 11:17:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/02 11:17:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 476.81616548715556\n",
      "Mean Absolute Error (MAE): 209.9168863050334\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TimeSeriesForecasting\").getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"onlineretail.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Data preprocessing: Convert date and aggregate data\n",
    "data = data.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"M/d/yy H:mm\"))\n",
    "aggregated_data = (\n",
    "    data.groupBy(\"InvoiceDate\", \"StockCode\")\n",
    "    .sum(\"Quantity\")\n",
    "    .withColumnRenamed(\"sum(Quantity)\", \"TotalDemand\")\n",
    "    .orderBy(\"InvoiceDate\")\n",
    ")\n",
    "\n",
    "# Filter popular product (highest demand)\n",
    "popular_product = aggregated_data.groupBy(\"StockCode\").sum(\"TotalDemand\").orderBy(col(\"sum(TotalDemand)\").desc()).first()\n",
    "product_code = popular_product[\"StockCode\"]\n",
    "filtered_data = aggregated_data.filter(col(\"StockCode\") == product_code).orderBy(\"InvoiceDate\")\n",
    "\n",
    "# Feature Engineering: Create lag features\n",
    "window = Window.orderBy(\"InvoiceDate\")\n",
    "for lag_days in range(1, 4):  # Creating Lag_1, Lag_2, Lag_3\n",
    "    filtered_data = filtered_data.withColumn(f\"Lag_{lag_days}\", lag(\"TotalDemand\", lag_days).over(window))\n",
    "\n",
    "# Drop rows with null values due to lagging\n",
    "filtered_data = filtered_data.na.drop()\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[f\"Lag_{i}\" for i in range(1, 4)], outputCol=\"features\")\n",
    "prepared_data = assembler.transform(filtered_data).select(\"features\", col(\"TotalDemand\").alias(\"label\"))\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = prepared_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Data scaling\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(train_data)\n",
    "train_data = scaler_model.transform(train_data).select(col(\"scaledFeatures\").alias(\"features\"), \"label\")\n",
    "test_data = scaler_model.transform(test_data).select(col(\"scaledFeatures\").alias(\"features\"), \"label\")\n",
    "\n",
    "# Train a regression model (Linear Regression used here)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Hyperparameter Tuning with CrossValidator\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Train model\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Evaluate model on test data\n",
    "predictions = cv_model.transform(test_data)\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------+\n",
      "|MonthYear|StockCode|MonthlyDemand|\n",
      "+---------+---------+-------------+\n",
      "|  2010-12|    22637|          203|\n",
      "|  2010-12|    35957|          359|\n",
      "|  2010-12|    22651|          347|\n",
      "|  2010-12|   35591T|            3|\n",
      "|  2010-12|    70007|          418|\n",
      "|  2010-12|    47421|           72|\n",
      "|  2010-12|    22906|          212|\n",
      "|  2010-12|    21447|           22|\n",
      "|  2010-12|    22572|          261|\n",
      "|  2010-12|    22170|           71|\n",
      "|  2010-12|   85131A|           64|\n",
      "|  2010-12|    21849|           35|\n",
      "|  2010-12|    47579|            1|\n",
      "|  2010-12|   85025B|            1|\n",
      "|  2010-12|   15044C|           10|\n",
      "|  2010-12|    21943|            6|\n",
      "|  2010-12|   85036A|            9|\n",
      "|  2010-12|    22033|           48|\n",
      "|  2010-12|   84247L|           16|\n",
      "|  2010-12|    21317|            3|\n",
      "+---------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetail-DemandForecasting\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set legacy time parser policy to handle datetime parsing\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "file_path = \"Onlineretail.csv\"  # Replace with your file path\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Preprocess data\n",
    "# Convert InvoiceDate to Date type\n",
    "data = data.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"MM/dd/yy\"))\n",
    "\n",
    "# Add a column for Month-Year for demand aggregation\n",
    "data = data.withColumn(\"MonthYear\", date_format(col(\"InvoiceDate\"), \"yyyy-MM\"))\n",
    "\n",
    "# Aggregate the data to calculate monthly demand for each product\n",
    "monthly_demand = data.groupBy(\"MonthYear\", \"StockCode\") \\\n",
    "    .sum(\"Quantity\") \\\n",
    "    .withColumnRenamed(\"sum(Quantity)\", \"MonthlyDemand\") \\\n",
    "    .orderBy(\"MonthYear\")\n",
    "\n",
    "# Show aggregated data\n",
    "monthly_demand.show()\n",
    "\n",
    "# Step 4: Save the cleaned data for further analysis (optional)\n",
    "monthly_demand.write.csv(\"Monthly_Demand.csv\", header=True)\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 12:14:20 WARN Instrumentation: [0dc8eb75] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 8.922915890676034e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 4.484713280147685e-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel: uid=LinearRegression_775f44d04460, numFeatures=1\n",
      "Best Model RMSE: 0.004453997358378977\n",
      "Pipeline execution complete!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OnlineRetail-DemandForecasting\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set legacy time parser policy to handle datetime parsing\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "file_path = \"Onlineretail.csv\"  # Replace with your file path\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Preprocess data\n",
    "# Convert InvoiceDate to Date type\n",
    "data = data.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"MM/dd/yy\"))\n",
    "\n",
    "# Add a column for Month-Year for demand aggregation\n",
    "data = data.withColumn(\"MonthYear\", date_format(col(\"InvoiceDate\"), \"yyyy-MM\"))\n",
    "\n",
    "# Aggregate the data to calculate monthly demand for each product\n",
    "monthly_demand = data.groupBy(\"MonthYear\", \"StockCode\") \\\n",
    "    .sum(\"Quantity\") \\\n",
    "    .withColumnRenamed(\"sum(Quantity)\", \"MonthlyDemand\") \\\n",
    "    .orderBy(\"MonthYear\")\n",
    "\n",
    "# Step 4: Feature Engineering for Time Series\n",
    "# Prepare data for machine learning (convert to features and labels)\n",
    "assembler = VectorAssembler(inputCols=[\"MonthlyDemand\"], outputCol=\"features\")\n",
    "assembled_data = assembler.transform(monthly_demand)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaled_data = scaler.fit(assembled_data).transform(assembled_data)\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "train_data, test_data = scaled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 5: Linear Regression Model\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"MonthlyDemand\", maxIter=10)\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"MonthlyDemand\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"MonthlyDemand\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Step 6: Hyperparameter Tuning with CrossValidator\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Run cross-validation and select the best model\n",
    "cv_model = crossval.fit(train_data)\n",
    "best_model = cv_model.bestModel\n",
    "print(best_model)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_predictions = best_model.transform(test_data)\n",
    "best_rmse = evaluator.evaluate(best_predictions)\n",
    "print(f\"Best Model RMSE: {best_rmse}\")\n",
    "\n",
    "# Step 7: Save the best model (optional)\n",
    "# best_model.save(\"best_demand_forecasting_model\")\n",
    "\n",
    "print(\"Pipeline execution complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ace_tools in /opt/anaconda3/lib/python3.12/site-packages (0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ace_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ace_tools in /opt/anaconda3/lib/python3.12/site-packages (0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ace_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 12:42:58 WARN Instrumentation: [0c925972] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Initial RMSE  Initial MAE   Best RMSE   Best MAE\n",
      "0    270.171969   115.989885  270.218604  116.07705\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DemandForecastingModel\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set legacy parser policy to handle datetime formats\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Onlineretail.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"MM/dd/yy\"))\n",
    "\n",
    "# Filter out invalid data\n",
    "data = data.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
    "\n",
    "# Extract Month-Year and aggregate monthly demand for each product\n",
    "data = data.withColumn(\"MonthYear\", date_format(col(\"InvoiceDate\"), \"yyyy-MM\"))\n",
    "monthly_demand = data.groupBy(\"MonthYear\", \"StockCode\") \\\n",
    "    .sum(\"Quantity\") \\\n",
    "    .withColumnRenamed(\"sum(Quantity)\", \"MonthlyDemand\") \\\n",
    "    .orderBy(\"MonthYear\")\n",
    "\n",
    "# Feature engineering: Adding lag features\n",
    "window_spec = Window.partitionBy(\"StockCode\").orderBy(\"MonthYear\")\n",
    "monthly_demand = monthly_demand.withColumn(\"Lag1\", lag(\"MonthlyDemand\", 1).over(window_spec))\n",
    "monthly_demand = monthly_demand.withColumn(\"Lag2\", lag(\"MonthlyDemand\", 2).over(window_spec))\n",
    "monthly_demand = monthly_demand.dropna()\n",
    "\n",
    "# Prepare data for machine learning\n",
    "assembler = VectorAssembler(inputCols=[\"Lag1\", \"Lag2\"], outputCol=\"features\")\n",
    "prepared_data = assembler.transform(monthly_demand)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = prepared_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "scaled_train_data = scaler.fit(train_data).transform(train_data)\n",
    "scaled_test_data = scaler.fit(train_data).transform(test_data)\n",
    "\n",
    "# Step 2: Train a Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"MonthlyDemand\", maxIter=100)\n",
    "\n",
    "# Fit the model\n",
    "lr_model = lr.fit(scaled_train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model.transform(scaled_test_data)\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"MonthlyDemand\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"MonthlyDemand\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "# Hyperparameter tuning with CrossValidator\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator_rmse,\n",
    "                          numFolds=3)\n",
    "\n",
    "cv_model = crossval.fit(scaled_train_data)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Evaluate the best model\n",
    "best_predictions = best_model.transform(scaled_test_data)\n",
    "best_rmse = evaluator_rmse.evaluate(best_predictions)\n",
    "best_mae = evaluator_mae.evaluate(best_predictions)\n",
    "\n",
    "# Save results for reference\n",
    "results = {\n",
    "    \"Initial RMSE\": rmse,\n",
    "    \"Initial MAE\": mae,\n",
    "    \"Best RMSE\": best_rmse,\n",
    "    \"Best MAE\": best_mae\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame([results])\n",
    "\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 12:45:01 WARN Instrumentation: [96dee6af] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Results:\n",
      "Initial RMSE: 270.1719690365431\n",
      "Initial MAE: 115.98988502326142\n",
      "Best RMSE after Tuning: 270.2186044916398\n",
      "Best MAE after Tuning: 116.07705002550536\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DemandForecastingModel\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set legacy parser policy to handle datetime formats\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Onlineretail.csv\"\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"MM/dd/yy\"))\n",
    "\n",
    "# Filter out invalid data\n",
    "data = data.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
    "\n",
    "# Extract Month-Year and aggregate monthly demand for each product\n",
    "data = data.withColumn(\"MonthYear\", date_format(col(\"InvoiceDate\"), \"yyyy-MM\"))\n",
    "monthly_demand = data.groupBy(\"MonthYear\", \"StockCode\") \\\n",
    "    .sum(\"Quantity\") \\\n",
    "    .withColumnRenamed(\"sum(Quantity)\", \"MonthlyDemand\") \\\n",
    "    .orderBy(\"MonthYear\")\n",
    "\n",
    "# Feature engineering: Adding lag features\n",
    "window_spec = Window.partitionBy(\"StockCode\").orderBy(\"MonthYear\")\n",
    "monthly_demand = monthly_demand.withColumn(\"Lag1\", lag(\"MonthlyDemand\", 1).over(window_spec))\n",
    "monthly_demand = monthly_demand.withColumn(\"Lag2\", lag(\"MonthlyDemand\", 2).over(window_spec))\n",
    "monthly_demand = monthly_demand.dropna()\n",
    "\n",
    "# Prepare data for machine learning\n",
    "assembler = VectorAssembler(inputCols=[\"Lag1\", \"Lag2\"], outputCol=\"features\")\n",
    "prepared_data = assembler.transform(monthly_demand)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = prepared_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "scaled_train_data = scaler.fit(train_data).transform(train_data)\n",
    "scaled_test_data = scaler.fit(train_data).transform(test_data)\n",
    "\n",
    "# Step 2: Train a Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"MonthlyDemand\", maxIter=100)\n",
    "\n",
    "# Fit the model\n",
    "lr_model = lr.fit(scaled_train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model.transform(scaled_test_data)\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"MonthlyDemand\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"MonthlyDemand\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "# Hyperparameter tuning with CrossValidator\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator_rmse,\n",
    "                          numFolds=3)\n",
    "\n",
    "cv_model = crossval.fit(scaled_train_data)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Evaluate the best model\n",
    "best_predictions = best_model.transform(scaled_test_data)\n",
    "best_rmse = evaluator_rmse.evaluate(best_predictions)\n",
    "best_mae = evaluator_mae.evaluate(best_predictions)\n",
    "\n",
    "# Save results for reference\n",
    "results = {\n",
    "    \"Initial RMSE\": rmse,\n",
    "    \"Initial MAE\": mae,\n",
    "    \"Best RMSE\": best_rmse,\n",
    "    \"Best MAE\": best_mae\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(f\"Initial RMSE: {rmse}\")\n",
    "print(f\"Initial MAE: {mae}\")\n",
    "print(f\"Best RMSE after Tuning: {best_rmse}\")\n",
    "print(f\"Best MAE after Tuning: {best_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, when, lit\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, NGram\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load Dataset\n",
    "data = spark.read.csv(\"reviews.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Ensure required columns\n",
    "print(\"Dataset Columns:\", data.columns)\n",
    "\n",
    "# Add 'Product' column if missing\n",
    "if \"Product\" not in data.columns:\n",
    "    data = data.withColumn(\"Product\", lit(\"DefaultProduct\"))\n",
    "\n",
    "# Step 3: Text Preprocessing\n",
    "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"tokens\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "ngram = NGram(n=2, inputCol=\"filtered_tokens\", outputCol=\"ngrams\")\n",
    "vectorizer = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\")\n",
    "\n",
    "# Step 4: Label Encoding\n",
    "label_indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 6: Model Training (Logistic Regression)\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, ngram, vectorizer, label_indexer, lr])\n",
    "\n",
    "# Fit model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Step 7: Evaluate Model\n",
    "predictions = model.transform(test_data)\n",
    "correct_predictions = predictions.filter(col(\"label\") == col(\"prediction\")).count()\n",
    "accuracy = correct_predictions / test_data.count()\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Step 8: Map Predictions to Sentiment Labels\n",
    "def map_label_to_sentiment(pred):\n",
    "    return \"Positive\" if pred == 1.0 else \"Negative\" if pred == 0.0 else \"Neutral\"\n",
    "\n",
    "map_udf = udf(map_label_to_sentiment, StringType())\n",
    "predictions = predictions.withColumn(\"PredictedSentiment\", map_udf(col(\"prediction\")))\n",
    "\n",
    "# Step 9: Sentiment Scoring\n",
    "sentiment_score = predictions.groupBy(\"Product\").agg(\n",
    "    avg(when(col(\"PredictedSentiment\") == \"Positive\", 1)\n",
    "        .when(col(\"PredictedSentiment\") == \"Negative\", -1)\n",
    "        .otherwise(0)).alias(\"SentimentScore\")\n",
    ")\n",
    "\n",
    "# Step 10: Show Results\n",
    "sentiment_score.show()\n",
    "\n",
    "# Step 11: Save Model and Results\n",
    "model.write().overwrite().save(\"sentiment_analysis_model\")\n",
    "sentiment_score.write.csv(\"sentiment_score.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
